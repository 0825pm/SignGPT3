# SignGPT3 mT5 Sign LM v2 Training Config
# ========================================
# 
# mT5 Encoder + Light-T2M 최적화 + Cycle Consistency
#
# ★ mBART → mT5 변경점:
# 1. Text Encoder: mBART → mT5 (다국어 네이티브)
# 2. Contrastive → Cycle Consistency (M2T Decoder)
#
# ★ Light-T2M 구조:
# 1. LIMM (Local Information Modeling): 1D Conv
# 2. ATII (Adaptive Textual Information Injector)
# 3. LGL Pattern: Local → Global → Local
#
# 복사 위치: configs/sign_lm_mt5_v2.yaml
# 실행: python train.py --cfg configs/sign_lm_mt5_v2.yaml --nodebug

NAME: SignGPT3_lm_mt5_v2
ACCELERATOR: 'gpu'
NUM_NODES: 1
DEVICE: [0]

# =============================================================================
# LM Ablation Settings
# =============================================================================
lm_ablation:
  motion_holder_repeat: 4
  holder_num_in_input: 4
  motion_holder_seq_mode: 'withse'
  with_hid_norm: False
  with_vae_latent_norm: True
  multi_hidden: True
  guidance_scale: 1.0
  model_guidance_scale: 1.0
  diffusion_batch_mul: 4
  guidance_uncondp: 0.1         # ★ CFG training (10% dropout)
  predict_epsilon: True
  fake_latent_mode: 'learnable_zero'
  instruction_type: t2m
  mot_factor: 1.0
  attention_mode: 'all'
  
ABLATION:
  SKIP_CONNECT: True
  PE_TYPE: mld
  DIFF_PE_TYPE: mld
  VAE_TYPE: mld
  VAE_ARCH: encoder_decoder
  MLP_DIST: False
  IS_DIST: False
  PREDICT_EPSILON: True

# =============================================================================
# Training
# =============================================================================
TRAIN:
  STAGE: lm_pretrain
  instruction_type: t2m
  NUM_WORKERS: 8
  BATCH_SIZE: 48               # ★ mT5 + Cycle이지만 v2는 가벼움
  END_EPOCH: 300
  PRETRAINED_VAE: /home/user/Projects/research/MotionGPT3/experiments/motgpt/SignGPT3_vae_mld/checkpoints/last.ckpt

  LR_SCHEDULER:
    target: CosineAnnealingLR
    params:
      T_max: 300
      eta_min: 1e-6

  OPTIM:
    target: AdamW
    params:
      lr: 3e-5                 # ★ mT5 fine-tune용 낮은 LR
      betas: [0.9, 0.99]
      weight_decay: 0.01
    params_diff:
      lr: 1e-4                 # Diffusion head용
      betas: [0.9, 0.99]
      weight_decay: 0.0

EVAL:
  BATCH_SIZE: 32
  SPLIT: val

TEST:
  CHECKPOINTS: ''
  SPLIT: test
  BATCH_SIZE: 32
  REPLICATION_TIMES: 2

# =============================================================================
# Dataset - 3개 데이터셋 모두 사용 (다국어)
# =============================================================================
DATASET:
  target: motGPT.data.H2S.H2SDataModule
  NFEATS: 120
  CODE_PATH: TOKENS
  NJOINTS: 55
  JOINT_TYPE: smplx
  H2S:
    DATASET_NAME: how2sign_csl_phoenix
    ROOT: /home/user/Projects/research/SOKE/data/How2Sign
    CSL_ROOT: /home/user/Projects/research/SOKE/data/CSL-Daily
    PHOENIX_ROOT: /home/user/Projects/research/SOKE/data/Phoenix_2014T
    MEAN_PATH: /home/user/Projects/research/SOKE/data/CSL-Daily/mean.pt
    STD_PATH: /home/user/Projects/research/SOKE/data/CSL-Daily/std.pt
    UNIT_LEN: 4
    FPS: 25
    MAX_MOTION_LEN: 300
    MIN_MOTION_LEN: 40
    MAX_TEXT_LEN: 40
    PICK_ONE_TEXT: true

# =============================================================================
# Model
# =============================================================================
model:
  target: motGPT.models.motgpt.MotGPT
  params:
    condition: 'text'
    task: 't2m'
    lm: ${lm.mt5_sign_v2}
    motion_vae: ${vae.mldvae}
    stage: ${TRAIN.STAGE}
    debug: ${DEBUG}
    codebook_size: 512
    guidance_scale: ${lm_ablation.guidance_scale}

  # Diffusion loss config
  diff_loss:
    target: motGPT.diffusion.diffloss.DiffLoss
    params:
      width: 1024
      depth: 3
      multi_hidden: ${lm_ablation.multi_hidden}
      num_sampling_steps: '100'
      noise_schedule: 'scaled_linear'
      learn_sigma: False
      sigma_small: True
      grad_checkpointing: False

# =============================================================================
# mT5 Sign LM v2 Config (Light-T2M 최적화)
# =============================================================================
lm:
  mt5_sign_v2:
    target: motGPT.archs.mt5_sign_lm_v2.MT5SignLMv2
    params:
      # ★★★ mT5 Encoder (다국어!) ★★★
      model_name: google/mt5-base        # 580M params, 768d
      model_type: mt5_sign_v2
      stage: ${TRAIN.STAGE}
      freeze_encoder: false              # ★ mT5 fine-tune!
      
      # Motion Branch (Light-T2M으로 간소화)
      motion_branch_layers: 4            # ★ 6 → 4 (LIMM이 대체)
      motion_branch_heads: 8
      hidden_dim: 768
      
      # ★★★ Light-T2M 최적화 ★★★
      use_limm: true                     # Local Information Modeling
      limm_kernel_size: 3                # 인접 3프레임 참조
      use_atii: true                     # Adaptive Text Injection
      
      # Shared Attention (Global) - 줄임
      num_shared_layers: 2               # ★ 4 → 2 (LIMM으로 보완)
      bidirectional_attention: true
      
      # ★★★ Cycle Consistency ★★★
      use_cycle_consistency: true
      cycle_weight: 0.1                  # L_total = L_diff + 0.1 * L_cycle
      m2t_decoder_layers: 4
      
      # Diffusion Head
      diffhead: ${model.diff_loss}
      vae_latent_channels: 256
      
      # CFG
      guidance_uncondp: ${lm_ablation.guidance_uncondp}
      guidance_scale: ${lm_ablation.guidance_scale}
      
      # Multi-hidden
      multi_hidden: ${lm_ablation.multi_hidden}
      
      # Compatible params
      motion_codebook_size: ${model.params.codebook_size}
      mot_factor: ${lm_ablation.mot_factor}
      attention_mode: ${lm_ablation.attention_mode}
      ablation: ${ABLATION}
      diffusion_batch_mul: ${lm_ablation.diffusion_batch_mul}
      predict_epsilon: ${lm_ablation.predict_epsilon}
      fake_latent_mode: ${lm_ablation.fake_latent_mode}
      motion_holder_repeat: ${lm_ablation.motion_holder_repeat}
      holder_num_in_input: ${lm_ablation.holder_num_in_input}
      motion_holder_seq_mode: ${lm_ablation.motion_holder_seq_mode}
      with_hid_norm: ${lm_ablation.with_hid_norm}
      with_vae_latent_norm: ${lm_ablation.with_vae_latent_norm}
      max_length: 128

# =============================================================================
# VAE Config (기존 SignGPT3 VAE)
# =============================================================================
vae:
  mldvae:
    target: motGPT.archs.mld_vae.MldVae
    params:
      arch: 'encoder_decoder'
      ff_size: 1024
      num_layers: 9
      num_heads: 4
      dropout: 0.1
      normalize_before: false
      activation: 'gelu'
      position_embedding: 'learned'
      latent_dim: [1, 256]
      code_num: 512
      ablation: ${ABLATION}
      nfeats: ${DATASET.NFEATS}

# =============================================================================
# Loss
# =============================================================================
LOSS:
  LAMBDA_FEATURE: 1.0
  LAMBDA_VELOCITY: 0.0
  LAMBDA_COMMIT: 0.02
  LAMBDA_CLS: 0.0
  LAMBDA_DIFF: 1.0
  ABLATION:
    RECONS_LOSS: 'l1_smooth'

# =============================================================================
# Metrics
# =============================================================================
METRIC:
  TYPE: ['MRMetrics']
  DIST_SYNC_ON_STEP: True

# =============================================================================
# Logger
# =============================================================================
LOGGER:
  TYPE: ['tensorboard', 'wandb']
  VAL_EVERY_STEPS: 10
  WANDB:
    params:
      tags: ['sign', 'mt5', 'light_t2m', 'cycle_consistency', 'multilingual']
      project: signgpt3

# =============================================================================
# Global Settings
# =============================================================================
FOLDER: experiments/signgpt3
DEBUG: False
SEED_VALUE: 1234
TINY: False
FULL_CONFIG: false