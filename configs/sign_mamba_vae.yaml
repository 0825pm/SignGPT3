# SignGPT3 Mamba VAE Training Config
# ====================================
#
# Light-T2M의 Mamba 구조를 차용한 경량 VAE
# MldVae 대비 ~2x 빠른 학습/추론 속도
#
# 실행: python train.py --cfg configs/sign_mamba_vae.yaml --nodebug
#
# 주요 변경점 (vs sign_vae.yaml):
# - target: motGPT.archs.mamba_vae.MambaVae
# - num_layers: 9 → 4 (Mamba가 효율적)
# - Mamba-specific params: d_state, d_conv, expand
#
# Author: SignGPT3 Team

NAME: SignGPT3_mamba_vae
DEBUG: False
ACCELERATOR: 'gpu'
NUM_NODES: 1
DEVICE: [0]

# =============================================================================
# Training
# =============================================================================
TRAIN:
  STAGE: vae
  NUM_WORKERS: 8
  BATCH_SIZE: 512              # Mamba는 메모리 효율적 → 배치 증가 가능
  END_EPOCH: 2000
  accumulate_grad_batches: 1
  RESUME: ''
  PRETRAINED: ''
  PRETRAINED_VAE: ''

  OPTIM:
    target: AdamW
    params:
      lr: 2e-4
      betas: [0.9, 0.99]
      weight_decay: 0.0

  LR_SCHEDULER:
    target: CosineAnnealingLR
    params:
      T_max: 2000
      eta_min: 1e-6

EVAL:
  BATCH_SIZE: 64
  SPLIT: val

TEST:
  CHECKPOINTS: ''
  SPLIT: test
  BATCH_SIZE: 64
  SAVE_PREDICTIONS: false

# =============================================================================
# Dataset - 3개 수어 데이터셋
# =============================================================================
DATASET:
  target: motGPT.data.H2S.H2SDataModule
  NFEATS: 120                   # SOKE 120-dim
  CODE_PATH: TOKENS
  NJOINTS: 55
  JOINT_TYPE: smplx
  H2S:
    DATASET_NAME: how2sign_csl_phoenix
    ROOT: /home/user/Projects/research/SOKE/data/How2Sign
    CSL_ROOT: /home/user/Projects/research/SOKE/data/CSL-Daily
    PHOENIX_ROOT: /home/user/Projects/research/SOKE/data/Phoenix_2014T
    MEAN_PATH: /home/user/Projects/research/SOKE/data/CSL-Daily/mean.pt
    STD_PATH: /home/user/Projects/research/SOKE/data/CSL-Daily/std.pt
    UNIT_LEN: 4
    FPS: 25
    MAX_MOTION_LEN: 300
    MIN_MOTION_LEN: 40
    MAX_TEXT_LEN: 40
    PICK_ONE_TEXT: true

# =============================================================================
# Metrics
# =============================================================================
METRIC:
  TYPE: ['MRMetrics']

# =============================================================================
# Loss
# =============================================================================
LOSS:
  LAMBDA_REC: 1.0
  LAMBDA_FEATURE: 1.0
  LAMBDA_VELOCITY: 0.0
  LAMBDA_KL: 1e-5
  LAMBDA_LATENT: 1e-5
  LAMBDA_COMMIT: 0.02
  LAMBDA_CLS: 1.0
  LAMBDA_DIFF: 1.0
  ABLATION:
    RECONS_LOSS: l1_smooth

# =============================================================================
# Model
# =============================================================================
model:
  target: motGPT.models.motgpt.MotGPT
  params:
    lm: null
    motion_vae: ${vae.mambavae}
    stage: vae
    debug: ${DEBUG}
    task: t2m
    condition: text
    metrics_dict: ${METRIC.TYPE}
    guidance_scale: 1.0
    codebook_size: 512

# =============================================================================
# VAE Config (MambaVae - Mamba-based Lightweight VAE)
# =============================================================================
vae:
  mambavae:
    target: motGPT.archs.mamba_vae.MambaVae
    params:
      nfeats: ${DATASET.NFEATS}          # 120
      latent_dim: [1, 256]               # [latent_size, latent_dim]
      
      # Mamba architecture params
      num_layers: 4                       # ★ 9 → 4 (Mamba는 더 효율적)
      num_groups: 16                      # GroupNorm groups
      d_state: 16                         # Mamba state dimension
      d_conv: 4                           # Mamba conv kernel
      expand: 2                           # Mamba expansion factor
      dropout: 0.1
      
      # MldVae 호환성을 위한 params (무시됨)
      ff_size: 1024
      num_heads: 4
      arch: encoder_decoder
      normalize_before: false
      activation: gelu
      position_embedding: learned
      
      datatype: h2s
      ablation: ${ABLATION}

# =============================================================================
# Ablation Settings (MldVae 호환)
# =============================================================================
ABLATION:
  VAE_TYPE: mamba                         # ★ mld → mamba
  VAE_ARCH: encoder_decoder
  PE_TYPE: mld
  DIFF_PE_TYPE: mld
  SKIP_CONNECT: True
  MLP_DIST: False
  IS_DIST: False
  PREDICT_EPSILON: True

# =============================================================================
# Logger
# =============================================================================
LOGGER:
  TYPE: ['tensorboard']
  VAL_EVERY_STEPS: 1
  TENSORBOARD:
    target: pytorch_lightning.loggers.TensorBoardLogger
    params:
      save_dir: experiments
      name: motgpt
      version: ${NAME}
  WANDB:
    target: pytorch_lightning.loggers.WandbLogger
    params:
      project: signgpt3
      name: ${NAME}
      save_dir: experiments

SEED_VALUE: 1234
FULL_CONFIG: false
